```{r echo=FALSE}
yml_content <- yaml::read_yaml("chapterauthors.yml")
author <- yml_content[["google-earth-engine-image-classification"]][["author"]]
```
# Image Classification and Introduction to Google Earth Engine {#google-earth-engine-image-classification}

Written by
```{r results='asis', echo=FALSE}
cat(author)
```

## Lab Overview {-}

Wide area remote sensing requires large-scale data storage and computation, which are often limiting factors for studies with large spatial extent or temporal repetition. One solution to the challenge of big data is cloud computing, which allows data storage and processing tasks to happen on remote supercomputing systems (i.e. not on your personal laptop). Google Earth Engine (GEE) provides an interface with which users can efficiently interact with large spatial datasets, including the entire Landsat satellite image archive.

In their 2017 paper, Gorelick et al. describe GEE as “a multi-petabyte analysis-ready data catalog co-located with a high performance, intrinsically parallel computation service”. Essentially, GEE is an online platform that enables analyses on large, internally-stored datasets. This technology has revolutionized the manner in which remote sensing can be conducted, presenting anyone with internet access the opportunity to participate in earth observation science.

------------------------------------------------------------------------

## Learning Objectives {-}

-  Understand applications of land cover mapping
-  Introduction to Google Earth Engine for image processing tasks  
-  Learn how to calculate simple spectral indices
-  Use a supervised classification to create a land cover map 

------------------------------------------------------------------------

## Deliverables {-}

-   Answers to 7 questions posed in the handout
-   A screenshot of the final land cover classification
-   Annotated code used to perform the analysis

------------------------------------------------------------------------

## Data {-}

Data will be accessed through the Google Earth Engine code editor. There are thousands of geospatial datasets stored within the GEE catalog. Landsat, Sentinel-1 and many other Earth-observing imagery are included with various levels of pre-processing in order to expedite analyses. Land cover, climate and other relevant environmental datasets are also available. 

------------------------------------------------------------------------

## Task 1: Getting started with Google Earth Engine {.unnumbered}

**Step 1:** It is recommended that you use/register for a gmail account prior to applying for access to GEE. To protect your privacy, you can anonymize your account by creating a username that is not attached to your actual name/student ID. Take note of your username and password. Once you have a gmail account, you need to sign-up for GEE using the following link: https://signup.earthengine.google.com/

Upon acceptance you will receive a confirmation email. Congratulations, you are now ready to apply your coding and geospatial knowledge in the Earth Engine Code Editor. GEE runs using Javascript, but don't worry you will be given templates for all the code you will need to complete the lab. 

``` {r, echo=FALSE, out.width = "75%", fig.align = 'center'}
knitr::include_graphics('images/02/GEE.jpg')
```

The GEE Code editor is divided into four panels: 

- The leftmost panel is where you can access saved scripts, view documentation and access spatial data you have saved to the Cloud.
- The middle panel is where you will write and run your code. 
- The right panel is where you will see printed results and outputs. 
- The bottom panel shows the map and is where you will display spatial data and imagery.

**Step 2:** We will practice selecting and displaying a cloud-free Landsat scene over Vancovuer.
First, we will need to define a Region of Interest to crop our Landsat scene to.

In the Map panel, click on the Pin icon and create a point in Maple Ridge. You will see that this point is automatically named and will appear in the code editor with the variable name 'geometry'. 

``` {r, echo=FALSE, out.width = "75%", fig.align = 'center'}
knitr::include_graphics('images/02/geometry.jpg')
```

The following codeblock selects a cloud-free image from the Landsat 8 Surface Reflectance archive and saves it as a new variable called `image`. Each line in codeblock applies criteria to filter the image archive: 

- filter by location - only include images that intersect with the region of interest
- filter by date - include images acquired between two dates
- sort the remaining images by the amount of cloud cover
- finally, select the first image in the list, which will have the least cloudy pixels

The `print` function will print the image metadata in the Console. 

The `Map.addLayer()` function displays the multi-band image in the Map and applies a min/max filter. 
The `//` symbols allow you to comment the code, these lines do not run anything but can help you organize code and understand what is going on. 

    // Filter an image collection by location, date and cloud cover
    var image = ee.Image(ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')
        .filterBounds(geometry)
        .filterDate('2020-06-01', '2022-08-31')
        .sort('CLOUD_COVER')
        .first()); // select the least cloudy image
    
    // Add the image to the Map    
    print(image);
    Map.addLayer(image, {bands: ["SR_B4","SR_B3","SR_B2"], min:7000, max: 15000}, 'filteredScene');

You should now see the following image displayed on the Map.

Expand the image properties in the Console tab and answer the following questions: 

``` {r, echo=FALSE, out.width = "75%", fig.align = 'center'}
knitr::include_graphics('images/02/bbysfirstScene.jpg')
```

##### Q1: What UTM Zone is the image in? What date was the image acquired? How much cloud cover is present over the land? {.unnumbered}

------------------------------------------------------------------------

## Task 2: Calculating NDVI  {.unnumbered}

In this task, you will calculate the Normalized Difference Vegetation Index. In the next task you will use it to perform a supervised classification and produce a map of land cover. 

Spectral indices combine spectral reflectance from two or more bands to highlight areas of spectral importance in an image. There are a wide variety of spectral indices used to identify a different land covers and image properties including Normalized Difference Vegetation Index (NDVI) Normalized Burn Ratio (NBR) and Normalized Difference Water Index to name a few. 

NDVI is calculated with the following formula: 

$\ NDVI = \frac{(NIR - RED)}{(NIR+RED)}$

Where NIR is the near-infrared band and red bands. The results of this equation should be between -1 and 1 with values less than 0 representing water and values between 0-1 representing different levels of green vegetation. 

**Step 1:** Use the following codeblock to calculate NDVI using band 5 (NIR) and band 4 (RED), and display it on the map: 

    var red = image.select('SR_B4');
    var nir = image.select('SR_B5');
    var ndvi = nir.subtract(red).divide(nir.add(red)).rename('NDVI');
    
    print(ndvi);
    Map.addLayer(ndvi, {min:0, max: 1}, 'NDVI');

##### Q2. Take a screenshot of NDVI over the UBC campus.{.unnumbered}

##### Q3. Explain how red and NIR reflectance is different for healthy vegetation. How does NDVI use this difference to estimate productivity? {.unnumbered}

Open the Inspector panel and click on pixels in the image to print their values: 

``` {r, echo=FALSE, out.width = "75%", fig.align = 'center'}
knitr::include_graphics('images/02/inspector.jpg')
```

##### Q4. Use the Inspector tool to sample pixels in water, vegetation and urban areas: approximately what range of NDVI values do you observe for these land cover types? {.unnumbered}

------------------------------------------------------------------------

## Task 3: Perform a Supervised Classification  {.unnumbered}

In the section you will use a Supervised Classification to classify the following land cover types: 

- Urban
- Water
- Forest
- Agriculture

**Step 1:** First we will create a sample of pixels to train the classification on. In the Geometry Imports menu, click **+new layer**. Add a new layer for each of the four land cover classes, rename each layer and assign it a color using the gear icon. 

In the settings tab (accessed from the gear icon), change Import as to **Feature Collection.** Add a new property called **landcover** and fill it with a **numeric value** unique to each land cover type. For example, Water = 0, Forest = 1, Urban = 2 etc...

``` {r, echo=FALSE, out.width = "75%", fig.align = 'center'}
knitr::include_graphics('images/02/training.jpg')
```

Select a land cover class in the Geometry Imports menu and use the point and polygon tools to select pixels in the image representing that class. Toggle the NDVI and true color composite on/off to aid your land cover interpretation. The example below gives an idea of how many pixels you want to sample per class. 

``` {r, echo=FALSE, fig.align = 'center'}
knitr::include_graphics('images/02/sample.jpg')
```

**Step 2:**Once you have selected training areas for each land cover class use the following code block to merge the Feature Collections and select a sample of pixels to train the classification model. The training pixels will be stored as a new variable called `training`.

    var classNames = Water.merge(Urban).merge(Agriculture).merge(Forest);
    print(classNames);
    
    var bands = ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'NDVI'];
    var training = image.select(bands).sampleRegions({
      collection: classNames,
      properties: ['landcover'],
      scale: 30
    });

##### Q5: Explain how a supervised and unsupervised classification are different? {.unnumbered}

**Step 3:** Next we will use the **Minimum Distance** algorithm to train our classifier and predict land cover for all the pixels in our image. 

First we make a new variable called `classifier` which contains the minimum distance classifier trained on our sample pixels. Conceptually, for a given pixel the model takes into account reflectance from the 6 spectral bands and NDVI and compares it with statistics generated from the training sample. The pixel is then assigned to the land cover class that it is most similar to.

Finally, we apply the model to the entire image and save it as a new layer called `classified`. 

    var classifier = ee.Classifier.minimumDistance().train({
      features: training,
      classProperty: 'landcover',
      inputProperties: bands
    });
    
    var classified = image.select(bands).classify(classifier);
    Map.addLayer(classified,
    {min: 0, max: 3, palette: ['orange', 'green', 'blue','yellow']},
    'classification');    
    
**Step 4:** After you click Run, zoom into the greater Vancouver Area. The new classification layer will take some time to run and be displayed on the map. If you are having trouble viewing the classified layer, zoom in more so that there is less area to compute over.  

**Depending on the numeric values you assigned each land cover class, you may want to reorder the palette colors so that each color is assigned to an appropriate class**

##### Q6: Include a screenshot of the final classified layer (see example below). Include a legend for the land cover types. What classes do you think were classified the most and least accurately? Why do you think some classes are more likely to be misclassified? {.unnumbered}

``` {r, echo=FALSE, out.width = "75%", fig.align = 'center'}
knitr::include_graphics('images/02/classified.jpg')
```

##### Q7: Use comments `//` to annotate each block of code you used to run this analysis. Your annotation should be brief, but explain what each processing step does. Use the first annotated code block for as example. Include you final annotated code in your deliverables. {.unnumbered}